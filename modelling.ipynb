{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/cesar/.local/lib/python3.10/site-packages (1.25.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/cesar/.local/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/cesar/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/cesar/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/cesar/.local/lib/python3.10/site-packages (from pandas) (1.25.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/cesar/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/cesar/.local/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/cesar/.local/lib/python3.10/site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /home/cesar/.local/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: plotly in /home/cesar/.local/lib/python3.10/site-packages (5.15.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/cesar/.local/lib/python3.10/site-packages (from plotly) (8.2.2)\n",
      "Requirement already satisfied: packaging in /home/cesar/.local/lib/python3.10/site-packages (from plotly) (23.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /home/cesar/.local/lib/python3.10/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/cesar/.local/lib/python3.10/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/cesar/.local/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/cesar/.local/lib/python3.10/site-packages (from matplotlib) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/cesar/.local/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/cesar/.local/lib/python3.10/site-packages (from matplotlib) (1.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/cesar/.local/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/cesar/.local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/cesar/.local/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/cesar/.local/lib/python3.10/site-packages (from scikit-learn) (1.25.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/cesar/.local/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/cesar/.local/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/cesar/.local/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /home/cesar/.local/lib/python3.10/site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /home/cesar/.local/lib/python3.10/site-packages (from seaborn) (1.25.0)\n",
      "Requirement already satisfied: pandas>=0.25 in /home/cesar/.local/lib/python3.10/site-packages (from seaborn) (2.0.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /home/cesar/.local/lib/python3.10/site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/cesar/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/cesar/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/cesar/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/cesar/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/cesar/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/cesar/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=0.25->seaborn) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/cesar/.local/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install nltk\n",
    "# !pip install plotly\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-learn\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#from sklearn.feature_extraction import stop_words\n",
    "from nltk.text import Text\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from pandas import DataFrame\n",
    "from plotly import graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# WORKDIR = 'C:/Users/thewr/git/sentiment_analysis_twitter_netflix/'\n",
    "WORKDIR = '/home/cesar/BD/sentiment_analysis_twitter_netflix/'\n",
    "\n",
    "netflix_tweets_proc_file = WORKDIR + '/Data/Processed/netflix_all_tweets.parquet'\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_netflix_all_tweets \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetflix_tweets_proc_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print('shape:', df_netflix_all_tweets.shape)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print('columns:', df_netflix_all_tweets.columns)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parquet.py:493\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[1;32m    430\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    437\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    438\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;124;03m    DataFrame\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    496\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    497\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parquet.py:60\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     58\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "df_netflix_all_tweets = pd.read_parquet(netflix_tweets_proc_file)\n",
    "\n",
    "# print('shape:', df_netflix_all_tweets.shape)\n",
    "# print('columns:', df_netflix_all_tweets.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reaction_to_status_id</th>\n",
       "      <th>reaction_to_status_text</th>\n",
       "      <th>reaction_to_status_created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>url</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>sentimento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H</td>\n",
       "      <td>Mon Feb 01 13:12:55 +0000 2021</td>\n",
       "      <td>1356229083223515136</td>\n",
       "      <td>@NetflixBrasil oie https://t.co/33L2geGxWD</td>\n",
       "      <td>Mon Feb 01 13:13:05 +0000 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mencao_arroba  oie</td>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H</td>\n",
       "      <td>Mon Feb 01 13:12:55 +0000 2021</td>\n",
       "      <td>1356229130489126920</td>\n",
       "      <td>@NetflixBrasil J谩 ia falar que vcs estavam atrasados </td>\n",
       "      <td>Mon Feb 01 13:13:16 +0000 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mencao_arroba  ja ia falar que vcs estavam atrasados  rosto_chorando_de_rir</td>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  reaction_to_status_id  \\\n",
       "0   1356229042647789569   \n",
       "1   1356229042647789569   \n",
       "\n",
       "                                                     reaction_to_status_text  \\\n",
       "0  Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H   \n",
       "1  Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H   \n",
       "\n",
       "    reaction_to_status_created_at                   id  \\\n",
       "0  Mon Feb 01 13:12:55 +0000 2021  1356229083223515136   \n",
       "1  Mon Feb 01 13:12:55 +0000 2021  1356229130489126920   \n",
       "\n",
       "                                                   text_x  \\\n",
       "0              @NetflixBrasil oie https://t.co/33L2geGxWD   \n",
       "1  @NetflixBrasil J谩 ia falar que vcs estavam atrasados    \n",
       "\n",
       "                       created_at  url  \\\n",
       "0  Mon Feb 01 13:13:05 +0000 2021  NaN   \n",
       "1  Mon Feb 01 13:13:16 +0000 2021  NaN   \n",
       "\n",
       "                                                                    text_cleaned  \\\n",
       "0                                                           mencao_arroba  oie     \n",
       "1   mencao_arroba  ja ia falar que vcs estavam atrasados  rosto_chorando_de_rir    \n",
       "\n",
       "  in_reply_to_status_id_str quoted_status_id_str  sentimento  \n",
       "0       1356229042647789569                 None         0.0  \n",
       "1       1356229042647789569                 None         2.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_netflix_all_tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thewr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\thewr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stopwords_list = nltk.corpus.stopwords.words('english') + nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "newStopWords = ['pra', 'diaz','juliette','lumena','carla','bbb21','gilberto','victor', 'hugo', 'globo', 'rodolfo', 'thais...','tha铆s','thais', 'caio', 'arcrebiano', 'brother',\n",
    "               'lucas', 'bbb2021', 'pared茫o', 'probosta','projota','lider','l铆der', 'karol', 'conka', 'pocah', 'penteado', 'rodolffo','camilla','sarah','nego',\n",
    "               'fiuk','fiukk','fiu','arthur','viih','boninho','big','pocahh','bbb']\n",
    "final_stopwords_list.extend(newStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reaction_to_status_id</th>\n",
       "      <th>reaction_to_status_text</th>\n",
       "      <th>reaction_to_status_created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>url</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>sentimento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H</td>\n",
       "      <td>Mon Feb 01 13:12:55 +0000 2021</td>\n",
       "      <td>1356229083223515136</td>\n",
       "      <td>@NetflixBrasil oie https://t.co/33L2geGxWD</td>\n",
       "      <td>Mon Feb 01 13:13:05 +0000 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mencao_arroba  oie</td>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H</td>\n",
       "      <td>Mon Feb 01 13:12:55 +0000 2021</td>\n",
       "      <td>1356229130489126920</td>\n",
       "      <td>@NetflixBrasil J谩 ia falar que vcs estavam atrasados </td>\n",
       "      <td>Mon Feb 01 13:13:16 +0000 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mencao_arroba  ja ia falar que vcs estavam atrasados  rosto_chorando_de_rir</td>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  reaction_to_status_id  \\\n",
       "0   1356229042647789569   \n",
       "1   1356229042647789569   \n",
       "\n",
       "                                                     reaction_to_status_text  \\\n",
       "0  Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H   \n",
       "1  Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H   \n",
       "\n",
       "    reaction_to_status_created_at                   id  \\\n",
       "0  Mon Feb 01 13:12:55 +0000 2021  1356229083223515136   \n",
       "1  Mon Feb 01 13:12:55 +0000 2021  1356229130489126920   \n",
       "\n",
       "                                                   text_x  \\\n",
       "0              @NetflixBrasil oie https://t.co/33L2geGxWD   \n",
       "1  @NetflixBrasil J谩 ia falar que vcs estavam atrasados    \n",
       "\n",
       "                       created_at  url  \\\n",
       "0  Mon Feb 01 13:13:05 +0000 2021  NaN   \n",
       "1  Mon Feb 01 13:13:16 +0000 2021  NaN   \n",
       "\n",
       "                                                                    text_cleaned  \\\n",
       "0                                                           mencao_arroba  oie     \n",
       "1   mencao_arroba  ja ia falar que vcs estavam atrasados  rosto_chorando_de_rir    \n",
       "\n",
       "  in_reply_to_status_id_str quoted_status_id_str  sentimento  \n",
       "0       1356229042647789569                 None         0.0  \n",
       "1       1356229042647789569                 None         2.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_netflix_all_tweets.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentilex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Sentilex lexicon with polarity words\n",
    "sentilexpt = open('SentiLex-lem-PT02.txt')\n",
    "\n",
    "#Creating dictionary with polarity words\n",
    "dic_polarity_word = {}\n",
    "for i in sentilexpt.readlines():\n",
    "  pos_point = i.find('.')\n",
    "  word = (i[:pos_point])\n",
    "  pol_pos = i.find('POL')\n",
    "  polarity = (i[pol_pos+7:pol_pos+9]).replace(';', '')\n",
    "  dic_polarity_word[word] = polarity\n",
    "\n",
    "#print (dic_polarity_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_value_prediction(text):\n",
    "    text = text.lower()\n",
    "    l_sentiment = []    \n",
    "\n",
    "    for p in text.split():    \n",
    "        l_sentiment.append(int(dic_polarity_word.get(p, 0)))\n",
    "    \n",
    "    value = sum(l_sentiment)                    \n",
    "    \n",
    "    \n",
    "    if value > 0:\n",
    "        return 2.0\n",
    "    elif value == 0:\n",
    "        return 0.0\n",
    "    elif value < 0:\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "df_netflix_all_tweets['sentiment_sentilex']= df_netflix_all_tweets['text_x'].apply(lambda x:sentiment_value_prediction(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reaction_to_status_id</th>\n",
       "      <th>reaction_to_status_text</th>\n",
       "      <th>reaction_to_status_created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>url</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>sentimento</th>\n",
       "      <th>sentiment_sentilex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H</td>\n",
       "      <td>Mon Feb 01 13:12:55 +0000 2021</td>\n",
       "      <td>1356229083223515136</td>\n",
       "      <td>@NetflixBrasil oie https://t.co/33L2geGxWD</td>\n",
       "      <td>Mon Feb 01 13:13:05 +0000 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mencao_arroba  oie</td>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H</td>\n",
       "      <td>Mon Feb 01 13:12:55 +0000 2021</td>\n",
       "      <td>1356229130489126920</td>\n",
       "      <td>@NetflixBrasil J谩 ia falar que vcs estavam atrasados </td>\n",
       "      <td>Mon Feb 01 13:13:16 +0000 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mencao_arroba  ja ia falar que vcs estavam atrasados  rosto_chorando_de_rir</td>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  reaction_to_status_id  \\\n",
       "0   1356229042647789569   \n",
       "1   1356229042647789569   \n",
       "\n",
       "                                                     reaction_to_status_text  \\\n",
       "0  Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H   \n",
       "1  Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H   \n",
       "\n",
       "    reaction_to_status_created_at                   id  \\\n",
       "0  Mon Feb 01 13:12:55 +0000 2021  1356229083223515136   \n",
       "1  Mon Feb 01 13:12:55 +0000 2021  1356229130489126920   \n",
       "\n",
       "                                                   text_x  \\\n",
       "0              @NetflixBrasil oie https://t.co/33L2geGxWD   \n",
       "1  @NetflixBrasil J谩 ia falar que vcs estavam atrasados    \n",
       "\n",
       "                       created_at  url  \\\n",
       "0  Mon Feb 01 13:13:05 +0000 2021  NaN   \n",
       "1  Mon Feb 01 13:13:16 +0000 2021  NaN   \n",
       "\n",
       "                                                                    text_cleaned  \\\n",
       "0                                                           mencao_arroba  oie     \n",
       "1   mencao_arroba  ja ia falar que vcs estavam atrasados  rosto_chorando_de_rir    \n",
       "\n",
       "  in_reply_to_status_id_str quoted_status_id_str  sentimento  \\\n",
       "0       1356229042647789569                 None         0.0   \n",
       "1       1356229042647789569                 None         2.0   \n",
       "\n",
       "   sentiment_sentilex  \n",
       "0                 0.0  \n",
       "1                 0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_netflix_all_tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Sentiment Distribution - Sentilex Lexicon')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment distribution according to Sentilex\n",
    "plt.figure(figsize=(12,6))\n",
    "ax = sns.countplot(x='sentiment_sentilex',palette=['gray','red','green'], data=df_netflix_all_tweets)\n",
    "ax.set_xticklabels(['Neutral','Negative','Positive'], rotation='horizontal', fontsize=10)\n",
    "ax.set_title('Sentiment Distribution - Sentilex Lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41909049977487617"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(df_netflix_all_tweets['sentimento'], df_netflix_all_tweets['sentiment_sentilex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.90      0.57      9019\n",
      "         1.0       0.29      0.28      0.29      1627\n",
      "         2.0       0.64      0.06      0.11     11564\n",
      "\n",
      "    accuracy                           0.42     22210\n",
      "   macro avg       0.45      0.41      0.32     22210\n",
      "weighted avg       0.52      0.42      0.31     22210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(df_netflix_all_tweets['sentimento'], df_netflix_all_tweets['sentiment_sentilex']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predito    0.0   1.0   2.0    All\n",
      "Real                             \n",
      "0.0       8135   560   324   9019\n",
      "1.0       1087   457    83   1627\n",
      "2.0      10304   544   716  11564\n",
      "All      19526  1561  1123  22210\n"
     ]
    }
   ],
   "source": [
    "print (pd.crosstab(df_netflix_all_tweets['sentimento'], df_netflix_all_tweets['sentiment_sentilex'], rownames=['Real'], colnames=['Predito'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LeIA is a fork from tool and lexicon VADER (Valence Aware Dictionary and sEntiment Reasoner) adapted to portuguese texts\n",
    "#https://github.com/rafjaa/LeIA\n",
    "import leia\n",
    "from leia import SentimentIntensityAnalyzer\n",
    "\n",
    "def sentiment_prediction_leia(text):\n",
    "    \n",
    "    s = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    polarity = s.polarity_scores(text)\n",
    "    return polarity\n",
    "   \n",
    "\n",
    "df_netflix_all_tweets['sentiment_leia_compound']= df_netflix_all_tweets['text_cleaned'].apply(lambda x:sentiment_prediction_leia(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reaction_to_status_id</th>\n",
       "      <th>reaction_to_status_text</th>\n",
       "      <th>reaction_to_status_created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>url</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>sentimento</th>\n",
       "      <th>sentiment_sentilex</th>\n",
       "      <th>sentiment_leia_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H</td>\n",
       "      <td>Mon Feb 01 13:12:55 +0000 2021</td>\n",
       "      <td>1356229083223515136</td>\n",
       "      <td>@NetflixBrasil oie https://t.co/33L2geGxWD</td>\n",
       "      <td>Mon Feb 01 13:13:05 +0000 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mencao_arroba  oie</td>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H</td>\n",
       "      <td>Mon Feb 01 13:12:55 +0000 2021</td>\n",
       "      <td>1356229130489126920</td>\n",
       "      <td>@NetflixBrasil J谩 ia falar que vcs estavam atrasados </td>\n",
       "      <td>Mon Feb 01 13:13:16 +0000 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mencao_arroba  ja ia falar que vcs estavam atrasados  rosto_chorando_de_rir</td>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  reaction_to_status_id  \\\n",
       "0   1356229042647789569   \n",
       "1   1356229042647789569   \n",
       "\n",
       "                                                     reaction_to_status_text  \\\n",
       "0  Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H   \n",
       "1  Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H   \n",
       "\n",
       "    reaction_to_status_created_at                   id  \\\n",
       "0  Mon Feb 01 13:12:55 +0000 2021  1356229083223515136   \n",
       "1  Mon Feb 01 13:12:55 +0000 2021  1356229130489126920   \n",
       "\n",
       "                                                   text_x  \\\n",
       "0              @NetflixBrasil oie https://t.co/33L2geGxWD   \n",
       "1  @NetflixBrasil J谩 ia falar que vcs estavam atrasados    \n",
       "\n",
       "                       created_at  url  \\\n",
       "0  Mon Feb 01 13:13:05 +0000 2021  NaN   \n",
       "1  Mon Feb 01 13:13:16 +0000 2021  NaN   \n",
       "\n",
       "                                                                    text_cleaned  \\\n",
       "0                                                           mencao_arroba  oie     \n",
       "1   mencao_arroba  ja ia falar que vcs estavam atrasados  rosto_chorando_de_rir    \n",
       "\n",
       "  in_reply_to_status_id_str quoted_status_id_str  sentimento  \\\n",
       "0       1356229042647789569                 None         0.0   \n",
       "1       1356229042647789569                 None         2.0   \n",
       "\n",
       "   sentiment_sentilex                                sentiment_leia_compound  \n",
       "0                 0.0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "1                 0.0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_netflix_all_tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extractig only de compound value from 'sentiment_leia_compound' column\n",
    "df_netflix_all_tweets['sentiment_leia_number'] = df_netflix_all_tweets['sentiment_leia_compound'].apply(pd.Series)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reaction_to_status_id</th>\n",
       "      <th>reaction_to_status_text</th>\n",
       "      <th>reaction_to_status_created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>url</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>sentimento</th>\n",
       "      <th>sentiment_sentilex</th>\n",
       "      <th>sentiment_leia_compound</th>\n",
       "      <th>sentiment_leia_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H</td>\n",
       "      <td>Mon Feb 01 13:12:55 +0000 2021</td>\n",
       "      <td>1356229083223515136</td>\n",
       "      <td>@NetflixBrasil oie https://t.co/33L2geGxWD</td>\n",
       "      <td>Mon Feb 01 13:13:05 +0000 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mencao_arroba  oie</td>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H</td>\n",
       "      <td>Mon Feb 01 13:12:55 +0000 2021</td>\n",
       "      <td>1356229130489126920</td>\n",
       "      <td>@NetflixBrasil J谩 ia falar que vcs estavam atrasados </td>\n",
       "      <td>Mon Feb 01 13:13:16 +0000 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mencao_arroba  ja ia falar que vcs estavam atrasados  rosto_chorando_de_rir</td>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  reaction_to_status_id  \\\n",
       "0   1356229042647789569   \n",
       "1   1356229042647789569   \n",
       "\n",
       "                                                     reaction_to_status_text  \\\n",
       "0  Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H   \n",
       "1  Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H   \n",
       "\n",
       "    reaction_to_status_created_at                   id  \\\n",
       "0  Mon Feb 01 13:12:55 +0000 2021  1356229083223515136   \n",
       "1  Mon Feb 01 13:12:55 +0000 2021  1356229130489126920   \n",
       "\n",
       "                                                   text_x  \\\n",
       "0              @NetflixBrasil oie https://t.co/33L2geGxWD   \n",
       "1  @NetflixBrasil J谩 ia falar que vcs estavam atrasados    \n",
       "\n",
       "                       created_at  url  \\\n",
       "0  Mon Feb 01 13:13:05 +0000 2021  NaN   \n",
       "1  Mon Feb 01 13:13:16 +0000 2021  NaN   \n",
       "\n",
       "                                                                    text_cleaned  \\\n",
       "0                                                           mencao_arroba  oie     \n",
       "1   mencao_arroba  ja ia falar que vcs estavam atrasados  rosto_chorando_de_rir    \n",
       "\n",
       "  in_reply_to_status_id_str quoted_status_id_str  sentimento  \\\n",
       "0       1356229042647789569                 None         0.0   \n",
       "1       1356229042647789569                 None         2.0   \n",
       "\n",
       "   sentiment_sentilex                                sentiment_leia_compound  \\\n",
       "0                 0.0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}   \n",
       "1                 0.0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}   \n",
       "\n",
       "   sentiment_leia_number  \n",
       "0                    0.0  \n",
       "1                    0.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_netflix_all_tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapting leIa sentiment number to compare with 'sentimento' column\n",
    "df_netflix_all_tweets['sentiment_leia_number_to_compare_metrics'] = df_netflix_all_tweets['sentiment_leia_number'].apply(lambda x: 2.0 if x > 0 else 1.0 if x < 0 else 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reaction_to_status_id</th>\n",
       "      <th>reaction_to_status_text</th>\n",
       "      <th>reaction_to_status_created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>url</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>sentimento</th>\n",
       "      <th>sentiment_sentilex</th>\n",
       "      <th>sentiment_leia_compound</th>\n",
       "      <th>sentiment_leia_number</th>\n",
       "      <th>sentiment_leia_number_to_compare_metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H</td>\n",
       "      <td>Mon Feb 01 13:12:55 +0000 2021</td>\n",
       "      <td>1356229083223515136</td>\n",
       "      <td>@NetflixBrasil oie https://t.co/33L2geGxWD</td>\n",
       "      <td>Mon Feb 01 13:13:05 +0000 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mencao_arroba  oie</td>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H</td>\n",
       "      <td>Mon Feb 01 13:12:55 +0000 2021</td>\n",
       "      <td>1356229130489126920</td>\n",
       "      <td>@NetflixBrasil J谩 ia falar que vcs estavam atrasados </td>\n",
       "      <td>Mon Feb 01 13:13:16 +0000 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mencao_arroba  ja ia falar que vcs estavam atrasados  rosto_chorando_de_rir</td>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  reaction_to_status_id  \\\n",
       "0   1356229042647789569   \n",
       "1   1356229042647789569   \n",
       "\n",
       "                                                     reaction_to_status_text  \\\n",
       "0  Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H   \n",
       "1  Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H   \n",
       "\n",
       "    reaction_to_status_created_at                   id  \\\n",
       "0  Mon Feb 01 13:12:55 +0000 2021  1356229083223515136   \n",
       "1  Mon Feb 01 13:12:55 +0000 2021  1356229130489126920   \n",
       "\n",
       "                                                   text_x  \\\n",
       "0              @NetflixBrasil oie https://t.co/33L2geGxWD   \n",
       "1  @NetflixBrasil J谩 ia falar que vcs estavam atrasados    \n",
       "\n",
       "                       created_at  url  \\\n",
       "0  Mon Feb 01 13:13:05 +0000 2021  NaN   \n",
       "1  Mon Feb 01 13:13:16 +0000 2021  NaN   \n",
       "\n",
       "                                                                    text_cleaned  \\\n",
       "0                                                           mencao_arroba  oie     \n",
       "1   mencao_arroba  ja ia falar que vcs estavam atrasados  rosto_chorando_de_rir    \n",
       "\n",
       "  in_reply_to_status_id_str quoted_status_id_str  sentimento  \\\n",
       "0       1356229042647789569                 None         0.0   \n",
       "1       1356229042647789569                 None         2.0   \n",
       "\n",
       "   sentiment_sentilex                                sentiment_leia_compound  \\\n",
       "0                 0.0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}   \n",
       "1                 0.0  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}   \n",
       "\n",
       "   sentiment_leia_number  sentiment_leia_number_to_compare_metrics  \n",
       "0                    0.0                                       0.0  \n",
       "1                    0.0                                       0.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_netflix_all_tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing columns of no use\n",
    "df_netflix_all_tweets.drop(('sentiment_leia_compound'),axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reaction_to_status_id</th>\n",
       "      <th>reaction_to_status_text</th>\n",
       "      <th>reaction_to_status_created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>url</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>sentimento</th>\n",
       "      <th>sentiment_sentilex</th>\n",
       "      <th>sentiment_leia_number</th>\n",
       "      <th>sentiment_leia_number_to_compare_metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H</td>\n",
       "      <td>Mon Feb 01 13:12:55 +0000 2021</td>\n",
       "      <td>1356229083223515136</td>\n",
       "      <td>@NetflixBrasil oie https://t.co/33L2geGxWD</td>\n",
       "      <td>Mon Feb 01 13:13:05 +0000 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mencao_arroba  oie</td>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H</td>\n",
       "      <td>Mon Feb 01 13:12:55 +0000 2021</td>\n",
       "      <td>1356229130489126920</td>\n",
       "      <td>@NetflixBrasil J谩 ia falar que vcs estavam atrasados </td>\n",
       "      <td>Mon Feb 01 13:13:16 +0000 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mencao_arroba  ja ia falar que vcs estavam atrasados  rosto_chorando_de_rir</td>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  reaction_to_status_id  \\\n",
       "0   1356229042647789569   \n",
       "1   1356229042647789569   \n",
       "\n",
       "                                                     reaction_to_status_text  \\\n",
       "0  Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H   \n",
       "1  Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H   \n",
       "\n",
       "    reaction_to_status_created_at                   id  \\\n",
       "0  Mon Feb 01 13:12:55 +0000 2021  1356229083223515136   \n",
       "1  Mon Feb 01 13:12:55 +0000 2021  1356229130489126920   \n",
       "\n",
       "                                                   text_x  \\\n",
       "0              @NetflixBrasil oie https://t.co/33L2geGxWD   \n",
       "1  @NetflixBrasil J谩 ia falar que vcs estavam atrasados    \n",
       "\n",
       "                       created_at  url  \\\n",
       "0  Mon Feb 01 13:13:05 +0000 2021  NaN   \n",
       "1  Mon Feb 01 13:13:16 +0000 2021  NaN   \n",
       "\n",
       "                                                                    text_cleaned  \\\n",
       "0                                                           mencao_arroba  oie     \n",
       "1   mencao_arroba  ja ia falar que vcs estavam atrasados  rosto_chorando_de_rir    \n",
       "\n",
       "  in_reply_to_status_id_str quoted_status_id_str  sentimento  \\\n",
       "0       1356229042647789569                 None         0.0   \n",
       "1       1356229042647789569                 None         2.0   \n",
       "\n",
       "   sentiment_sentilex  sentiment_leia_number  \\\n",
       "0                 0.0                    0.0   \n",
       "1                 0.0                    0.0   \n",
       "\n",
       "   sentiment_leia_number_to_compare_metrics  \n",
       "0                                       0.0  \n",
       "1                                       0.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_netflix_all_tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Sentiment Distribution - Leia Lexicon')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment distribution according to LeIa\n",
    "plt.figure(figsize=(12,6))\n",
    "ax = sns.countplot(x='sentiment_leia_number_to_compare_metrics',palette=['gray','red','green'],data=df_netflix_all_tweets)\n",
    "ax.set_xticklabels(['Neutral','Negative','Positive'], rotation='horizontal', fontsize=10)\n",
    "ax.set_title('Sentiment Distribution - Leia Lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4115263394867177"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(df_netflix_all_tweets['sentimento'], df_netflix_all_tweets['sentiment_leia_number_to_compare_metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.64      0.51      9019\n",
      "         1.0       0.20      0.53      0.29      1627\n",
      "         2.0       0.60      0.22      0.32     11564\n",
      "\n",
      "    accuracy                           0.41     22210\n",
      "   macro avg       0.41      0.46      0.37     22210\n",
      "weighted avg       0.50      0.41      0.39     22210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(df_netflix_all_tweets['sentimento'], df_netflix_all_tweets['sentiment_leia_number_to_compare_metrics']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predito    0.0   1.0   2.0    All\n",
      "Real                             \n",
      "0.0       5777  1798  1444   9019\n",
      "1.0        526   868   233   1627\n",
      "2.0       7320  1749  2495  11564\n",
      "All      13623  4415  4172  22210\n"
     ]
    }
   ],
   "source": [
    "print (pd.crosstab(df_netflix_all_tweets['sentimento'], df_netflix_all_tweets['sentiment_leia_number_to_compare_metrics'], rownames=['Real'], colnames=['Predito'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing data to apply other classification algorithms\n",
    "stemmer_ptbr = RSLPStemmer()\n",
    "def stemming_tokenizer(raw_text):     \n",
    "    words = raw_text.split() \n",
    "    words = [word for word in words if word not in final_stopwords_list]\n",
    "    words = [stemmer_ptbr.stem(word) for word in words]\n",
    "    return words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reaction_to_status_id</th>\n",
       "      <th>reaction_to_status_text</th>\n",
       "      <th>reaction_to_status_created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>url</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>sentimento</th>\n",
       "      <th>sentiment_sentilex</th>\n",
       "      <th>sentiment_leia_number</th>\n",
       "      <th>sentiment_leia_number_to_compare_metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H</td>\n",
       "      <td>Mon Feb 01 13:12:55 +0000 2021</td>\n",
       "      <td>1356229083223515136</td>\n",
       "      <td>@NetflixBrasil oie https://t.co/33L2geGxWD</td>\n",
       "      <td>Mon Feb 01 13:13:05 +0000 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mencao_arroba  oie</td>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H</td>\n",
       "      <td>Mon Feb 01 13:12:55 +0000 2021</td>\n",
       "      <td>1356229130489126920</td>\n",
       "      <td>@NetflixBrasil J谩 ia falar que vcs estavam atrasados </td>\n",
       "      <td>Mon Feb 01 13:13:16 +0000 2021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mencao_arroba  ja ia falar que vcs estavam atrasados  rosto_chorando_de_rir</td>\n",
       "      <td>1356229042647789569</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  reaction_to_status_id  \\\n",
       "0   1356229042647789569   \n",
       "1   1356229042647789569   \n",
       "\n",
       "                                                     reaction_to_status_text  \\\n",
       "0  Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H   \n",
       "1  Em fevereiro n茫o posso, t么 com a agenda cheia.  https://t.co/IBI89vqg5H   \n",
       "\n",
       "    reaction_to_status_created_at                   id  \\\n",
       "0  Mon Feb 01 13:12:55 +0000 2021  1356229083223515136   \n",
       "1  Mon Feb 01 13:12:55 +0000 2021  1356229130489126920   \n",
       "\n",
       "                                                   text_x  \\\n",
       "0              @NetflixBrasil oie https://t.co/33L2geGxWD   \n",
       "1  @NetflixBrasil J谩 ia falar que vcs estavam atrasados    \n",
       "\n",
       "                       created_at  url  \\\n",
       "0  Mon Feb 01 13:13:05 +0000 2021  NaN   \n",
       "1  Mon Feb 01 13:13:16 +0000 2021  NaN   \n",
       "\n",
       "                                                                    text_cleaned  \\\n",
       "0                                                           mencao_arroba  oie     \n",
       "1   mencao_arroba  ja ia falar que vcs estavam atrasados  rosto_chorando_de_rir    \n",
       "\n",
       "  in_reply_to_status_id_str quoted_status_id_str  sentimento  \\\n",
       "0       1356229042647789569                 None         0.0   \n",
       "1       1356229042647789569                 None         2.0   \n",
       "\n",
       "   sentiment_sentilex  sentiment_leia_number  \\\n",
       "0                 0.0                    0.0   \n",
       "1                 0.0                    0.0   \n",
       "\n",
       "   sentiment_leia_number_to_compare_metrics  \n",
       "0                                       0.0  \n",
       "1                                       0.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_netflix_all_tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_netflix_all_tweets['sentimento']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22210, 634)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(tokenizer=stemming_tokenizer, min_df=0.001, max_df=0.99)\n",
    "X = vectorizer_tfidf.fit_transform(df_netflix_all_tweets['text_cleaned'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thewr\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vocab = np.array(vectorizer_tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting dataframe into training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randon Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_features=0.5, min_samples_leaf=3, n_estimators=40,\n",
       "                       n_jobs=-1, oob_score=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = m.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 0., ..., 2., 0., 2.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importances = m.feature_importances_\n",
    "# sem_importancia = []\n",
    "# for i,w in enumerate(importances):\n",
    "#   if w == 0:\n",
    "#     sem_importancia.append(vocab[i])\n",
    "#     print(vocab[i],w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importances = m.feature_importances_\n",
    "# c_importancia = []\n",
    "# for i,w in enumerate(importances):\n",
    "#   if w > 0:\n",
    "#     c_importancia.append(vocab[i])\n",
    "#     print(vocab[i],w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(c_importancia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(sem_importancia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7926330150068213"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.87      0.78      2995\n",
      "         1.0       0.78      0.42      0.55       555\n",
      "         2.0       0.89      0.78      0.83      3780\n",
      "\n",
      "    accuracy                           0.79      7330\n",
      "   macro avg       0.79      0.69      0.72      7330\n",
      "weighted avg       0.81      0.79      0.79      7330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predito   0.0  1.0   2.0   All\n",
      "Real                          \n",
      "0.0      2615   48   332  2995\n",
      "1.0       279  235    41   555\n",
      "2.0       801   19  2960  3780\n",
      "All      3695  302  3333  7330\n"
     ]
    }
   ],
   "source": [
    "print (pd.crosstab(y_test, predicted, rownames=['Real'], colnames=['Predito'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(X_train, y_train) \n",
    "svm_predictions = svm_model_linear.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model accuracy for X_test   \n",
    "accuracy = svm_model_linear.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8004092769440655\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.92      0.79      2995\n",
      "         1.0       0.82      0.38      0.52       555\n",
      "         2.0       0.92      0.77      0.84      3780\n",
      "\n",
      "    accuracy                           0.80      7330\n",
      "   macro avg       0.82      0.69      0.72      7330\n",
      "weighted avg       0.82      0.80      0.80      7330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, svm_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predito   0.0  1.0   2.0   All\n",
      "Real                          \n",
      "0.0      2741   31   223  2995\n",
      "1.0       314  210    31   555\n",
      "2.0       850   14  2916  3780\n",
      "All      3905  255  3170  7330\n"
     ]
    }
   ],
   "source": [
    "print (pd.crosstab(y_test, svm_predictions, rownames=['Real'], colnames=['Predito'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7163710777626193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "knn = KNeighborsClassifier(n_neighbors = 7).fit(X_train, y_train) \n",
    "knn_predictions = knn.predict(X_test)\n",
    "# accuracy on X_test \n",
    "accuracy = knn.score(X_test, y_test) \n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.79      0.71      2995\n",
      "         1.0       0.78      0.24      0.37       555\n",
      "         2.0       0.80      0.73      0.76      3780\n",
      "\n",
      "    accuracy                           0.72      7330\n",
      "   macro avg       0.74      0.59      0.61      7330\n",
      "weighted avg       0.73      0.72      0.71      7330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, knn_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predito   0.0  1.0   2.0   All\n",
      "Real                          \n",
      "0.0      2366   26   603  2995\n",
      "1.0       331  133    91   555\n",
      "2.0      1016   12  2752  3780\n",
      "All      3713  171  3446  7330\n"
     ]
    }
   ],
   "source": [
    "print (pd.crosstab(y_test, knn_predictions, rownames=['Real'], colnames=['Predito'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a DescisionTreeClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "dtree_model = DecisionTreeClassifier(max_depth = 2).fit(X_train, y_train) \n",
    "dtree_predictions = dtree_model.predict(X_test) \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6244201909959072"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, dtree_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.99      0.68      2995\n",
      "         1.0       0.00      0.00      0.00       555\n",
      "         2.0       0.99      0.42      0.59      3780\n",
      "\n",
      "    accuracy                           0.62      7330\n",
      "   macro avg       0.50      0.47      0.43      7330\n",
      "weighted avg       0.72      0.62      0.59      7330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thewr\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\thewr\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\thewr\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, dtree_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predito   0.0   2.0   All\n",
      "Real                     \n",
      "0.0      2979    16  2995\n",
      "1.0       555     0   555\n",
      "2.0      2182  1598  3780\n",
      "All      5716  1614  7330\n"
     ]
    }
   ],
   "source": [
    "print (pd.crosstab(y_test, dtree_predictions, rownames=['Real'], colnames=['Predito'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40204638472032744\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB \n",
    "gnb = GaussianNB().fit(X_train.toarray(), y_train) \n",
    "gnb_predictions = gnb.predict(X_test.toarray()) \n",
    "  \n",
    "# accuracy on X_test \n",
    "accuracy = gnb.score(X_test.toarray(), y_test) \n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.13      0.21      2995\n",
      "         1.0       0.12      0.89      0.20       555\n",
      "         2.0       0.88      0.54      0.67      3780\n",
      "\n",
      "    accuracy                           0.40      7330\n",
      "   macro avg       0.52      0.52      0.36      7330\n",
      "weighted avg       0.69      0.40      0.45      7330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, gnb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predito  0.0   1.0   2.0   All\n",
      "Real                          \n",
      "0.0      396  2326   273  2995\n",
      "1.0       49   492    14   555\n",
      "2.0      269  1452  2059  3780\n",
      "All      714  4270  2346  7330\n"
     ]
    }
   ],
   "source": [
    "print (pd.crosstab(y_test, gnb_predictions, rownames=['Real'], colnames=['Predito'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM seems to be the best prediction algorithm with an acceptable accuracy of 0.81 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
